---
title: "NLP-CdMM"
author: "Mathilde"
date: "2023-01-13"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

I - Pr√©paration des donn√©es 

1. Je d√©finis ma session de travail

```{r}
setwd("~/Documents/GitHub/Distant Reading I/Cours_03")
monDossier="~/Users/mathildeschwoerer/Documents/GitHub/Distant Reading I/Cours_03/CdMM_NLP"
```

2. J'importe ma propre liste de _stopwords_.

````{r}
StopW <- "StopwordsLatin.txt"
StopW = read.csv(StopW, header=FALSE, stringsAsFactors=FALSE)[,]
head(StopW,10)
```

3. Nettoyage du texte


````{r}
df <- read.csv("cdmm.csv", sep=",")
#On cr√©e une cha√Æne de caract√®re vide qui contiendra √† l'avenir  le texte contenu dans df. 
texte_long <- ""
```

Puisqu'il est possible que ma liste de Stopwords contienne des majuscules malgr√© ma viligance, je m'assure de tout transformer en minuscules √† l'aide d'une fonction. 

````{r}
StopW <- tolower(StopW)
```

J'applique la r√©duction √† la minuscule √† chaque mot de mon tableau √† l'aide d'une boucle. Si le mot ne figure pas dans ma liste de stopwords, je l'ajoute √† la cha√Æne de caract√®re nomm√©e "texte_long" . Je le d√©barrasse √©galement de la ponctuation encombrante.

````{r}
for (word in tolower(df$lemma)) {
  if (!word %in% StopW) {
    texte_long <- paste(texte_long, word, sep=" ")
  }
  #On enl√®ve la ponctuation.
  texte_long <- gsub("[[:punct:]]", "", texte_long)
}
texte_long
```
4. Cr√©ation d'une liste

Je sectionne ce texte en 10 morceaux et je les mets dans une liste qui s'appelle "extraits".

```{r}
Nb_sequences <- 10
extraits <- strwrap(texte_long, nchar(texte_long) / Nb_sequences)
#J'affiche le huiti√®me extrait.
extraits[8]
```

5. Transformation en matrice vectorielle

```{r}
#J'appelle deux bo√Ætes √† outils qui nous seront utiles.
library("tm")
library("tidytext")

corpus <- Corpus(VectorSource(extraits), readerControl = list(language = "lat"))
# J'affiche les informations √† propos de ce corpus
corpus
```
Je compte le nombre de colonne de la matrice.

```{r}
ncol(as.matrix(DocumentTermMatrix(corpus)))
#Affichage du quatri√®me vecteur de la matrice.
corpus[[4]][[1]]
```
6. Cr√©ation d'un _document term matrix_

```{r}
dtmCdMM <- DocumentTermMatrix(corpus)
dtmCdMM
```
II - Analyse des donn√©es 

1. Connaissance des fr√©quences 
```{r}
freq <- as.data.frame(colSums(as.matrix(dtmCdMM)))
colnames(freq) <- c("frequence")
#Pour g√©n√©rer un graph, nous avons besoin de la library suivante :
library("ggplot2")
#G√©n√©ration du graph :
ggplot(freq, aes(x=frequence)) + geom_density()
```

Afin de mieux comprendre la constitution de mes donn√©es, je m'int√©resse aux mots peu fr√©quents. Comme mon texte ne comprend que 740 documents, je pose l'hypoth√®se de consid√©rer comme rares les mots qu'on trouve moins de deux fois. 

```{r}
#Cr√©ation d'une variable rep√©rant les mots qui n'apparaissent qu'une seule fois.
motsPeuFrequents <- findFreqTerms(dtmCdMM, 0, 1)
#On compte le nombre de mots concern√©s.
length(motsPeuFrequents)
#On affiche les cinquante premiers. 
head(motsPeuFrequents,50)
```

J'agis de m√™me avec les mots fr√©quents, ceux qui apparaissent deux fois ou plus.

```{r}
motsFrequents <- findFreqTerms(dtmCdMM, 2, Inf)
length(motsFrequents)
head(motsFrequents,50)
```
Consid√©rant que le nombre total de mots n'est pas suffisant pour me permettre de faire un grand m√©nage, je n'applique pas cette √©tape propos√©e par mon enseignant dans le script qui me sert d'appui. 

2. Topic modeling

```{r}
#Appel √† une library sp√©cialis√©e pour le topic-modeling.
library("topicmodels")
#On commence par tenter de d√©gager deux sujets...
k = 2
lda_2 <- LDA(dtmCdMM, k= k, control = list(seed = 1234))
#Puis trois. 
lda_3 <- LDA(dtmCdMM, k= k+1, control = list(alpha = 0.1))
```

On calcule le "beta" de chaque mot (voir Documentation).

```{r}
themes <- tidy(lda_2, matrix = "beta")
themes
```
On mobilise aussi l'√©chantillonnage de Gibbs qui repose sur un fonctionnemment it√©ratif. Il faut d√©finir ses param√®tres (nous conservons ceux de notre enseignant ainsi que ses commentaires).

```{r}
## Set parameters for Gibbs sampling
#Le mod√®le va tourner 2000 fois avant de commencer √† enregistrer les r√©sultats.
burnin <- 2000
#Apr√®s cela il va encore tourner 2000 fois.
iter <- 2000
# Il ne va enregistrer le r√©sultat que toutes les 500 it√©rations.
thin <- 500
#seed et nstart pour la reproductibilit√©
SEED=c(1, 2, 3, 4, 5)
seed <-SEED
nstart <- 5
#Seul meilleur mod√®le est utilis√©.
best <- TRUE
#2 topics
lda_gibbs_2 <- LDA(dtmCdMM, k, method="Gibbs", control=list(nstart=nstart, seed=seed, best=best, burnin=burnin, iter=iter, thin=thin))
#3 topics
lda_gibbs_3 <- LDA(dtmCdMM, k+1, method="Gibbs", control=list(nstart=nstart, seed=seed, best=best, burnin=burnin, iter=iter, thin=thin))
```

On cr√©e des dataframes pour rendre compte des op√©rations statistiques effectu√©es.

```{r}
"LDA 2"
termsTopic <- as.data.frame(terms(lda_2,10))
head(termsTopic,11)
"LDA 3"
termsTopic <- as.data.frame(terms(lda_3,10))
head(termsTopic,11)
"LDA GIBBS 2"
termsTopic <- as.data.frame(terms(lda_gibbs_2,10))
head(termsTopic,11)
#Ce dataframe nous para√Æt le plus repr√©sentatif du po√®me analys√©, aussi g√©n√©rons-nous un code pour l'int√©grer √† notre rapport LaTeX.
library(xtable)
print(xtable(termsTopic, type = "latex"), file = "LDAGibbs2.tex")
"LDA GIBBS 3"
termsTopic <- as.data.frame(terms(lda_gibbs_3,10))
head(termsTopic,11)
```

On associe les r√©sultats du mod√®le de Gibbs √† leur beta.

```{r}
topics <- tidy(lda_gibbs_2, matrix = "beta")
topics
```

III - Visualisation

```{r}
#Je vais solliciter une nouvelle biblioth√®que d'outils.
if (!require("dplyr")){
   install.packages("dplyr")
  library("dplyr")
}

#Je r√©cup√®re mes mots.
top_terms <- topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup()  %>%
  arrange(topic, -beta)
#J'√©labore un graph.
top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) + geom_col(show.legend = FALSE) +
                                                  facet_wrap(~ topic, scales = "free") +
                                                  coord_flip() +
                                                  scale_x_reordered()
```

On r√©cup√®re les mots pour les associer √† leur ùõÉ.

```{r, fig.width=20, fig.height=20}
tm <- posterior(lda_gibbs_2)$terms
data = data.frame(colnames(tm))
head(data)
```
On cr√©e un nuage de mots pour chaque th√®me d√©gag√©.

```{r, fig.width=30, fig.height=20}
#On appelle les bo√Ætes √† outils requises.
library("wordcloud")
library("RColorBrewer")
library("wordcloud2")

#On d√©finit les param√®tres des nuages de mots.
for(topic in seq(k)){
    data$topic <-tm[topic,]
    wordcloud(
      words = data$colnames.tm.,
      freq = data$topic,
      #Fr√©quence minimale que doit justifier le mot pour √™tre affich√©.
      min.freq=0.0002,
      #Nombre maximal de mots contenus dans chaque wordcloud.
      max.words=30,
      #On les classe par ordre croissant. 
      random.order=FALSE,
      #Proportion de mots √©crits √† 90%.
      rot.per=.35,
      #Taille du graphique.
      scale=c(10,10),
      #Palette de couleurs.
      colors = brewer.pal(5, "Set1")
    )
}
```